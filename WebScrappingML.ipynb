{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw0iU0spO64grDbbaNYsja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hllsweb/B.L.A-AI/blob/main/WebScrappingML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz8GPlNmlojK",
        "outputId": "18b6fc6b-a8e3-4d55-90c4-173400183978"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.2)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading automat-25.4.16-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (4.13.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.2.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.1.31)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading automat-25.4.16-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-25.4.16 constantly-23.10.4 cssselect-1.3.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.4.0 queuelib-1.8.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.3.0 w3lib-2.3.1 zope.interface-7.2\n"
          ]
        }
      ],
      "source": [
        "# Instalação das bibliotecas principais para web scraping avançado\n",
        "!pip install requests beautifulsoup4 lxml pandas tqdm nest_asyncio\n",
        "\n",
        "# Instalação do Scrapy (opcional, pode ser usado para crawling profundo)\n",
        "!pip install scrapy\n",
        "\n",
        "# (Opcional) Instalação do Selenium para páginas dinâmicas\n",
        "# !pip install selenium webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import lxml\n",
        "import nest_asyncio\n",
        "import scrapy\n",
        "\n",
        "print(\"Todas as bibliotecas foram importadas com sucesso!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbcaoL-0nSHS",
        "outputId": "f622d66d-7b0f-422a-c09b-4c8ae19cc0a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todas as bibliotecas foram importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "# Seeds definidas\n",
        "seeds = [\n",
        "    \"https://www.mercadolivre.com.br/ajuda/termos-e-condicoes-de-uso_991\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politica-de-privacidade_881\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politicas-de-anuncios_1122\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/o-que-e-o-sistema-de-reputacao_220\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/vender-no-mercado-livre_604\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politica-de-devolucao_630\"\n",
        "]\n",
        "\n",
        "def is_internal(url):\n",
        "    # Só considera links do domínio mercadolivre.com.br/ajuda\n",
        "    parsed = urlparse(url)\n",
        "    return parsed.netloc == \"www.mercadolivre.com.br\" and parsed.path.startswith(\"/ajuda\")\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\n\\n>>> Página: {seed}\")\n",
        "    try:\n",
        "        resp = requests.get(seed, timeout=10)\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Extrai título\n",
        "        title = soup.find(\"title\").text.strip() if soup.find(\"title\") else \"Sem título\"\n",
        "        print(f\"Título: {title}\\n\")\n",
        "\n",
        "        # Extrai texto principal (melhor heurística inicial: pega todo o texto da <main> ou <body>)\n",
        "        main = soup.find(\"main\") or soup.find(\"body\")\n",
        "        text = main.get_text(separator=\"\\n\", strip=True) if main else \"\"\n",
        "        print(f\"Texto (primeiros 400 caracteres):\\n{text[:400]}...\\n\")\n",
        "\n",
        "        # Extrai links internos únicos\n",
        "        links = set()\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            url = urljoin(seed, a['href'])\n",
        "            if is_internal(url):\n",
        "                links.add(url)\n",
        "        print(f\"Links internos encontrados ({len(links)}):\")\n",
        "        for l in list(links)[:10]:  # Mostra só os 10 primeiros\n",
        "            print(\" -\", l)\n",
        "        if len(links) > 10:\n",
        "            print(\" ...\")\n",
        "    except Exception as e:\n",
        "        print(\"Erro ao acessar:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0Rhkv8pnzqA",
        "outputId": "94a3fde5-f8ec-462b-95e1-c77f6bf97db1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            ">>> Página: https://www.mercadolivre.com.br/ajuda/termos-e-condicoes-de-uso_991\n",
            "Título: Termos e condições gerais de uso do site\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "Ajuda\n",
            "Termos e condições gerais de uso do site\n",
            "Buscar\n",
            "Termos e condições gerais de uso do site\n",
            "English Version\n",
            "Última modificação: 24 de novembro, 2023\n",
            "Resumo dos Termos e condições\n",
            "O Mercado Livre é uma empresa de tecnologia e oferece serviços relacionados, principalmente, com  comércio eletrônico e pagamentos digitais.\n",
            "O Marketplace é uma plataforma de comércio eletrônico onde as Pessoas Usuária...\n",
            "\n",
            "Links internos encontrados (17):\n",
            " - https://www.mercadolivre.com.br/ajuda/Termos-e-condicoes-gerais-de-uso_1409\n",
            " - https://www.mercadolivre.com.br/ajuda/Termos-e-condicoes-gerais-de-uso_1500\n",
            " - https://www.mercadolivre.com.br/ajuda/30487\n",
            " - https://www.mercadolivre.com.br/ajuda/1086\n",
            " - https://www.mercadolivre.com.br/ajuda/25781\n",
            " - https://www.mercadolivre.com.br/ajuda/299\n",
            " - https://www.mercadolivre.com.br/ajuda/termos-e-condicoes-de-uso_991#root-app\n",
            " - https://www.mercadolivre.com.br/ajuda#nav-header\n",
            " - https://www.mercadolivre.com.br/ajuda/30463\n",
            " - https://www.mercadolivre.com.br/ajuda/23300\n",
            " ...\n",
            "\n",
            "\n",
            ">>> Página: https://www.mercadolivre.com.br/ajuda/politica-de-privacidade_881\n",
            "Título: Error\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "...\n",
            "\n",
            "Links internos encontrados (5):\n",
            " - https://www.mercadolivre.com.br/ajuda/Termos-e-condicoes-gerais-de-uso_1409\n",
            " - https://www.mercadolivre.com.br/ajuda#nav-header\n",
            " - https://www.mercadolivre.com.br/ajuda\n",
            " - https://www.mercadolivre.com.br/ajuda/politica-de-privacidade_881#root-app\n",
            " - https://www.mercadolivre.com.br/ajuda/23303\n",
            "\n",
            "\n",
            ">>> Página: https://www.mercadolivre.com.br/ajuda/politicas-de-anuncios_1122\n",
            "Título: Error\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "...\n",
            "\n",
            "Links internos encontrados (5):\n",
            " - https://www.mercadolivre.com.br/ajuda/Termos-e-condicoes-gerais-de-uso_1409\n",
            " - https://www.mercadolivre.com.br/ajuda#nav-header\n",
            " - https://www.mercadolivre.com.br/ajuda/politicas-de-anuncios_1122#root-app\n",
            " - https://www.mercadolivre.com.br/ajuda\n",
            " - https://www.mercadolivre.com.br/ajuda/23303\n",
            "\n",
            "\n",
            ">>> Página: https://www.mercadolivre.com.br/ajuda/o-que-e-o-sistema-de-reputacao_220\n",
            "Título: Quais as tarifas de processamento com o Mercado Pago?\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "Ajuda\n",
            "Quais as tarifas de processamento com o Mercado Pago?\n",
            "Buscar\n",
            "Quais as tarifas de processamento com o Mercado Pago?\n",
            "As tarifas a pagar variam de acordo com:\n",
            "A ferramenta que você utiliza (maquininha Point, Point Tap, Código QR, Link de pagamento ou Checkout).\n",
            "O prazo escolhido para receber o dinheiro na sua conta.\n",
            "O meio de pagamento usado em cada venda.\n",
            "Você pode configurar os prazos de rece...\n",
            "\n",
            "Links internos encontrados (11):\n",
            " - https://www.mercadolivre.com.br/ajuda/3605\n",
            " - https://www.mercadolivre.com.br/ajuda/Termos-e-condicoes-gerais-de-uso_1409\n",
            " - https://www.mercadolivre.com.br/ajuda/24876\n",
            " - https://www.mercadolivre.com.br/ajuda/o-que-e-o-sistema-de-reputacao_220#root-app\n",
            " - https://www.mercadolivre.com.br/ajuda#nav-header\n",
            " - https://www.mercadolivre.com.br/ajuda\n",
            " - https://www.mercadolivre.com.br/ajuda/33392\n",
            " - https://www.mercadolivre.com.br/ajuda/33400\n",
            " - https://www.mercadolivre.com.br/ajuda/2609\n",
            " - https://www.mercadolivre.com.br/ajuda/23303\n",
            " ...\n",
            "\n",
            "\n",
            ">>> Página: https://www.mercadolivre.com.br/ajuda\n",
            "Título: Precisa de ajuda? Entre em contato conosco | Mercado Livre\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "Como podemos te ajudar?\n",
            "Buscar\n",
            "Compras\n",
            "Administrar e cancelar compras\n",
            "Pagar, rastrear envios, alterar, reclamar ou cancelar compras.\n",
            "Devoluções e reembolsos\n",
            "Devolver um produto ou consultar por estorno de compra.\n",
            "Perguntas frequentes sobre compras\n",
            "Vendas\n",
            "Reputação, vendas e envios\n",
            "Perguntar sobre sua reputação, envio, pagamento ou devolução.\n",
            "Gerenciar anúncios\n",
            "Melhorar a qualidade, alterar, exclui...\n",
            "\n",
            "Links internos encontrados (10):\n",
            " - https://www.mercadolivre.com.br/ajuda/Termos-e-condicoes-gerais-de-uso_1409\n",
            " - https://www.mercadolivre.com.br/ajuda/23303\n",
            " - https://www.mercadolivre.com.br/ajuda/18935\n",
            " - https://www.mercadolivre.com.br/ajuda/637\n",
            " - https://www.mercadolivre.com.br/ajuda#nav-header\n",
            " - https://www.mercadolivre.com.br/ajuda#root-app\n",
            " - https://www.mercadolivre.com.br/ajuda\n",
            " - https://www.mercadolivre.com.br/ajuda/643\n",
            " - https://www.mercadolivre.com.br/ajuda/Configuracao-da-minha-conta_662\n",
            " - https://www.mercadolivre.com.br/ajuda/seguranca_663\n",
            "\n",
            "\n",
            ">>> Página: https://www.mercadolivre.com.br/ajuda/vender-no-mercado-livre_604\n",
            "Título: Privacidade e confidencialidade do Mercado Pago\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "Ajuda\n",
            "Privacidade e confidencialidade do Mercado Pago\n",
            "Buscar\n",
            "Privacidade e confidencialidade do Mercado Pago\n",
            "English Version\n",
            "1. Políticas de privacidade e confidencialidade do Mercado Pago\n",
            "Leia com atenção estas Políticas de privacidade e confidencialidade antes de cadastrar-se ou mesmo de utilizar o Mercado Pago.\n",
            "Estas Políticas de privacidade e confidencialidade do Mercado Pago (doravante denomi...\n",
            "\n",
            "Links internos encontrados (5):\n",
            " - https://www.mercadolivre.com.br/ajuda/Termos-e-condicoes-gerais-de-uso_1409\n",
            " - https://www.mercadolivre.com.br/ajuda/vender-no-mercado-livre_604#root-app\n",
            " - https://www.mercadolivre.com.br/ajuda#nav-header\n",
            " - https://www.mercadolivre.com.br/ajuda\n",
            " - https://www.mercadolivre.com.br/ajuda/23303\n",
            "\n",
            "\n",
            ">>> Página: https://www.mercadolivre.com.br/ajuda/politica-de-devolucao_630\n",
            "Título: Error\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "...\n",
            "\n",
            "Links internos encontrados (5):\n",
            " - https://www.mercadolivre.com.br/ajuda/Termos-e-condicoes-gerais-de-uso_1409\n",
            " - https://www.mercadolivre.com.br/ajuda/politica-de-devolucao_630#root-app\n",
            " - https://www.mercadolivre.com.br/ajuda#nav-header\n",
            " - https://www.mercadolivre.com.br/ajuda\n",
            " - https://www.mercadolivre.com.br/ajuda/23303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "problematic_urls = [\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politica-de-privacidade_881\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politicas-de-anuncios_1122\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politica-de-devolucao_630\"\n",
        "]\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/122.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.8\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "    \"Connection\": \"keep-alive\"\n",
        "}\n",
        "\n",
        "for url in problematic_urls:\n",
        "    print(f\"\\n>>> Testando: {url}\")\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "        title = soup.find(\"title\").text.strip() if soup.find(\"title\") else \"Sem título\"\n",
        "        print(f\"Título: {title}\\n\")\n",
        "        main = soup.find(\"main\") or soup.find(\"body\")\n",
        "        text = main.get_text(separator=\"\\n\", strip=True) if main else \"\"\n",
        "        print(f\"Texto (primeiros 400 caracteres):\\n{text[:400]}...\\n\")\n",
        "    except Exception as e:\n",
        "        print(\"Erro:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o7RI5HwwvUg",
        "outputId": "1a0d76a7-1591-4387-efc0-5c7d3aa667e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Testando: https://www.mercadolivre.com.br/ajuda/politica-de-privacidade_881\n",
            "Título: Error\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "...\n",
            "\n",
            "\n",
            ">>> Testando: https://www.mercadolivre.com.br/ajuda/politicas-de-anuncios_1122\n",
            "Título: Error\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "...\n",
            "\n",
            "\n",
            ">>> Testando: https://www.mercadolivre.com.br/ajuda/politica-de-devolucao_630\n",
            "Título: Error\n",
            "\n",
            "Texto (primeiros 400 caracteres):\n",
            "...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalação de dependências do Selenium e ChromeDriver no Colab\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium webdriver-manager\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lICGoOxlw-GE",
        "outputId": "cfa1a523-3e78-408a-9e54-7a7390de4dd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.83)] [\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [2 InRelease 70.6 kB/128 kB 55%] [Waiting for headers] [Connected to cloud.r\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,543 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,154 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,272 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,604 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,696 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,845 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,111 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,841 kB]\n",
            "Fetched 30.6 MB in 4s (7,093 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 libudev1 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 snapd\n",
            "  squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 34 not upgraded.\n",
            "Need to get 30.3 MB of archives.\n",
            "After this operation, 123 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.15 [76.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.15 [1,557 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.67.1+22.04 [27.8 MB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 30.3 MB in 1s (44.7 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 126333 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.15_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.15) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.15) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126533 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.15_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.15) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.67.1+22.04_amd64.deb ...\n",
            "Unpacking snapd (2.67.1+22.04) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.15) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.67.1+22.04) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 126762 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.15) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.31.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.31.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, python-dotenv, outcome, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 python-dotenv-1.1.0 selenium-4.31.0 trio-0.30.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "\n",
        "urls = [\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politica-de-privacidade_881\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politicas-de-anuncios_1122\",\n",
        "    \"https://www.mercadolivre.com.br/ajuda/politica-de-devolucao_630\"\n",
        "]\n",
        "\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--disable-gpu\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument(\"--window-size=1920,1080\")\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "for url in urls:\n",
        "    print(f\"\\n>>> Testando (Selenium): {url}\")\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(3)  # Aguarda carregar JavaScript\n",
        "        title = driver.title\n",
        "        print(f\"Título: {title}\")\n",
        "        # Tenta pegar o texto do <main> ou <body>\n",
        "        try:\n",
        "            main = driver.find_element(By.TAG_NAME, \"main\")\n",
        "        except:\n",
        "            main = driver.find_element(By.TAG_NAME, \"body\")\n",
        "        text = main.text\n",
        "        print(f\"Texto (primeiros 400 caracteres):\\n{text[:400]}...\\n\")\n",
        "    except Exception as e:\n",
        "        print(\"Erro:\", e)\n",
        "\n",
        "driver.quit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C599fzcgxKjw",
        "outputId": "3fd0dda0-0696-48d2-e516-000bb74210ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Testando (Selenium): https://www.mercadolivre.com.br/ajuda/politica-de-privacidade_881\n",
            "Título: Error\n",
            "Texto (primeiros 400 caracteres):\n",
            "Parece que esta página não existe\n",
            "Se o problema persistir, você pode reportá-lo na seção Ajuda, usando o código a seguir:\n",
            "Código: \n",
            "404 \n",
            "Copiar\n",
            "Voltar...\n",
            "\n",
            "\n",
            ">>> Testando (Selenium): https://www.mercadolivre.com.br/ajuda/politicas-de-anuncios_1122\n",
            "Título: Error\n",
            "Texto (primeiros 400 caracteres):\n",
            "Parece que esta página não existe\n",
            "Se o problema persistir, você pode reportá-lo na seção Ajuda, usando o código a seguir:\n",
            "Código: \n",
            "404 \n",
            "Copiar\n",
            "Voltar...\n",
            "\n",
            "\n",
            ">>> Testando (Selenium): https://www.mercadolivre.com.br/ajuda/politica-de-devolucao_630\n",
            "Título: Error\n",
            "Texto (primeiros 400 caracteres):\n",
            "Parece que esta página não existe\n",
            "Se o problema persistir, você pode reportá-lo na seção Ajuda, usando o código a seguir:\n",
            "Código: \n",
            "404 \n",
            "Copiar\n",
            "Voltar...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Configurações iniciais\n",
        "START_URL = \"https://www.mercadolivre.com.br/ajuda\"\n",
        "DOMAIN = \"www.mercadolivre.com.br\"\n",
        "PATH_PREFIX = \"/ajuda\"\n",
        "MAX_DEPTH = 3  # Para evitar loops infinitos (ajuste se necessário)\n",
        "SLEEP_TIME = 0.7  # Para não sobrecarregar o site\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/122.0.0.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept-Language\": \"pt-BR,pt;q=0.9,en;q=0.8\"\n",
        "}\n",
        "\n",
        "def is_internal_url(url):\n",
        "    parsed = urlparse(url)\n",
        "    return parsed.netloc == DOMAIN and parsed.path.startswith(PATH_PREFIX)\n",
        "\n",
        "def is_error_page(soup):\n",
        "    # Detecta páginas de erro (404, etc)\n",
        "    if not soup.title:\n",
        "        return True\n",
        "    title = soup.title.text.strip().lower()\n",
        "    if 'error' in title or 'não existe' in title or '404' in title or 'não encontrada' in title:\n",
        "        return True\n",
        "    # Pode expandir checagens aqui\n",
        "    return False\n",
        "\n",
        "def extract_links(soup, base_url):\n",
        "    links = set()\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        url = urljoin(base_url, a['href'])\n",
        "        if is_internal_url(url):\n",
        "            links.add(url.split(\"#\")[0])  # Remove ancoras\n",
        "    return links\n",
        "\n",
        "def extract_text(soup):\n",
        "    main = soup.find(\"main\") or soup.find(\"body\")\n",
        "    if not main:\n",
        "        return \"\"\n",
        "    text = main.get_text(separator=\"\\n\", strip=True)\n",
        "    return text\n",
        "\n",
        "# Estruturas de controle\n",
        "visited = set()\n",
        "to_visit = [(START_URL, 0)]\n",
        "data = []\n",
        "\n",
        "with tqdm(total=1, desc=\"Crawling páginas\") as pbar:\n",
        "    while to_visit:\n",
        "        url, depth = to_visit.pop(0)\n",
        "        if url in visited or depth > MAX_DEPTH:\n",
        "            continue\n",
        "        try:\n",
        "            resp = requests.get(url, headers=headers, timeout=10)\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "            if is_error_page(soup):\n",
        "                visited.add(url)\n",
        "                continue\n",
        "            title = soup.title.text.strip() if soup.title else \"Sem título\"\n",
        "            text = extract_text(soup)\n",
        "            links = extract_links(soup, url)\n",
        "            data.append({\n",
        "                'url': url,\n",
        "                'titulo': title,\n",
        "                'texto': text,\n",
        "                'links_encontrados': list(links),\n",
        "                'profundidade': depth,\n",
        "            })\n",
        "            # Adiciona novos links na fila de visitação\n",
        "            for link in links:\n",
        "                if link not in visited:\n",
        "                    to_visit.append((link, depth+1))\n",
        "            visited.add(url)\n",
        "            pbar.total = pbar.total + len(links)\n",
        "            pbar.update(1)\n",
        "            time.sleep(SLEEP_TIME)\n",
        "        except Exception as e:\n",
        "            visited.add(url)\n",
        "            continue\n",
        "\n",
        "# Exporta para DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"\\nTotal de páginas coletadas: {len(df)}\")\n",
        "df.head()\n",
        "\n",
        "# (Opcional) Salva para CSV\n",
        "df.to_csv(\"mercadolivre_politicas_crawl.csv\", index=False)\n",
        "print(\"Arquivo salvo: mercadolivre_politicas_crawl.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubJ025Jb09OK",
        "outputId": "ff0f68d9-e19b-4336-a64b-0c034cbf877b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Crawling páginas:  14%|█▍        | 233/1656 [03:53<23:46,  1.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total de páginas coletadas: 233\n",
            "Arquivo salvo: mercadolivre_politicas_crawl.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leitura do csv no Colab\n",
        "df = pd.read_csv(\"mercadolivre_politicas_crawl.csv\")"
      ],
      "metadata": {
        "id": "MifncR7EH4gx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas requests beautifulsoup4 lxml tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT6gTe6zIVQ-",
        "outputId": "e743eb58-9829-486b-a86e-75075a25b952"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 mercadolivre.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF7_BdgOIZto",
        "outputId": "2ce82ead-b2f5-4068-88f6-c7fec39b32ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 233/233 [03:12<00:00,  1.21it/s]\n",
            "Extração web finalizada. Arquivo salvo: mercadolivre_politicas_full_extract.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 processa_base_ia.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGc3mdFYLBeZ",
        "outputId": "29d821a3-ca23-44f4-cfab-fc060c830eed"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 Lendo arquivo: mercadolivre_politicas_full_extract.csv\n",
            "🔹 Fragmentando textos em blocos estruturados...\n",
            "\r  0% 0/233 [00:00<?, ?it/s]\r100% 233/233 [00:00<00:00, 3578.40it/s]\n",
            "✅ Base de conhecimento estruturada salva em: mercadolivre_base_conhecimento_ia.csv\n",
            "Total de blocos gerados: 964\n",
            "                                             url  ...                                        texto_bloco\n",
            "53     https://www.mercadolivre.com.br/ajuda/299  ...  e)  Aderir a produtos e serviços diretamente n...\n",
            "857  https://www.mercadolivre.com.br/ajuda/29902  ...  FATTOR RECUPERACAO DE CREDITOS E GESTAO DE RIS...\n",
            "394  https://www.mercadolivre.com.br/ajuda/28247  ...  x) Animais domésticos ou não, roedores ou inse...\n",
            "\n",
            "[3 rows x 5 columns]\n"
          ]
        }
      ]
    }
  ]
}